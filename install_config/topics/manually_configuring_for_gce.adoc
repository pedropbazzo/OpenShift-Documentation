////
Module included in the following assemblies:

install_config/configuring_gce.adoc
////

[id='gce-configuring-masters-manual_{context}']
= Option 2: Manually configuring {product-title} for GCE

== Manually configuring master hosts for GCE

Perform the following procedure on all master hosts.

.Procedure

. Add the GCE parameters to the `apiServerArguments`
and `controllerArguments` sections of the master configuration file at `/etc/origin/master/master-config.yaml` by default:
+
[source,yaml]
----
apiServerArguments:
  cloud-provider:
    - "gce"
  cloud-config:
    - "/etc/origin/cloudprovider/gce.conf"
controllerArguments:
  cloud-provider:
    - "gce"
  cloud-config:
    - "/etc/origin/cloudprovider/gce.conf"
----

. When you configure {product-title} for GCP using Ansible, the
*_/etc/origin/cloudprovider/gce.conf_* file is created automatically. Because
you are manually configuring {product-title} for GCP, you must create the file
and enter the following:
+
[subs=+quotes]
----
[Global]
project-id = <project-id> <1>
network-name = <network-name> <2>
node-tags = <node-tags> <3>
node-instance-prefix = <instance-prefix> <4>
multizone = true <5>
----
<1> Provide the GCP project ID where the existing instances are running.
<2> Provide the network name if not using the default.
<3> Provide the tag for the GCP nodes. Must contain `ocp` as a suffix. For example, if the value of the `node-instance-prefix` parameter is set to `mycluster`, the nodes must be tagged with `myclusterocp`.
<4> Provide a unique string to identify your {product-title} cluster.
<5> Set to `true` to trigger multizone deployments on GCP. Set to `False` by
default.
+
The cluster installation process configures single-zone support by default.
+
Deploying {product-title} in GCP on different zones can be helpful to avoid
single-point-of-failures, but can cause problems with storage. This is because
GCP disks are created within a zone. If an {product-title} node host goes down
in zone "A" and the pods should be moved to zone "B", the persistent storage
cannot be attached to those pods, because the disks are now in a different zone.
See https://kubernetes.io/docs/admin/multiple-zones/#limitations[Multiple zone
limitations] in the Kubernetes documentation for more information.
+
[IMPORTANT]
====
For
xref:../configuring_gce.adoc#gce-load-balancer_configuring-for-GCE[running load balancer services
using GCP], the Compute Engine VM node instances require the `ocp` suffix:
`<openshift_gcp_prefix>ocp`. For example, if the value of the
`openshift_gcp_prefix` parameter is set to `mycluster`, you must tag the nodes
with `myclusterocp`. See
link:https://cloud.google.com/vpc/docs/add-remove-network-tags[Adding and
Removing Network Tags] for more information on how to add network tags to
Compute Engine VM instances.
====

. Restart the {product-title} host services:
+
[source,bash]
----
# master-restart api
# master-restart controllers
# systemctl restart atomic-openshift-node
----

To return to single-zone support, set the `multizone` value to `false` and
restart the master and node host services.


== Manually configuring node hosts for GCE

Perform the following on all node hosts.

.Procedure

. Edit the appropriate xref:../admin_guide/manage_nodes.adoc#modifying-nodes[node
configuration map] and update the contents of the `*kubeletArguments*`
section:
+
[source,yaml]
----
kubeletArguments:
  cloud-provider:
    - "gce"
  cloud-config:
    - "/etc/origin/cloudprovider/gce.conf"
----
+
[IMPORTANT]
====
The `nodeName` must match the instance name in GCP in order
for the cloud provider integration to work properly. The name must also be
RFC1123 compliant.
====

. Restart the {product-title} services on all nodes.
+
[source,bash]
----
# systemctl restart atomic-openshift-node
----
